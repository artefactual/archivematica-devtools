#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# This file is part of the Archivematica development tools.
#
# Copyright 2010-2016 Artefactual Systems Inc. <http://artefactual.com>
#
# Archivematica is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Archivematica is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Archivematica.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import print_function
import argparse
import sys

from elasticsearch import Elasticsearch, ElasticsearchException, NotFoundError
import elasticsearch.helpers

sys.path.append('/usr/lib/archivematica/archivematicaCommon')
import elasticSearchFunctions


def index_iterator(records, index, doc_type, action='index'):
    """
    Creates an iterator which yields dicts suitable for use with Elasticsearch's bulk operation API.
    Used by `reindex` to transform a list of _source records into actions ready for indexing.
    """
    for record in records:
        yield {
            '_op_type': action,
            '_index': index,
            '_type': doc_type,
            '_source': record,
        }


def reindex(index, doc_types, chunk_size):
    """
    Delete and recreate index/doc_types in Elasticsearch.

    :param str index: Index to recreate
    :param list doc_types: List of doc types to recreate
    :param int chunk_size: Number of docs in one chunk sent to Elasticsearch
    """
    try:  # Shim to handle elasticSearchFunctions cleanup
        conn = Elasticsearch(hosts=elasticSearchFunctions.getElasticsearchServerHostAndPort())
    except AttributeError:
        elasticSearchFunctions.setup_reading_from_client_conf()
        conn = elasticSearchFunctions.get_client()

    try:
        conn.indices.get_mapping(index)
    except NotFoundError:
        print("No", index, "mapping exists - exiting", file=sys.stderr)
        return 1

    doctype_records = {}
    # Fetch all records, store in memory
    for doc_type in doc_types:
        doctype_records[doc_type] = [r['_source'] for r in elasticsearch.helpers.scan(conn, index=index, doc_type=doc_type, scroll='15m')]

    # Delete exisitng index and recreate mapping
    conn.indices.delete(index=index)
    try:  # Shim to handle elasticSearchFunctions cleanup
        elasticSearchFunctions.connect_and_create_index(index)
    except AttributeError:
        elasticSearchFunctions.create_index(conn, index)

    # Reindex with the same data
    for doc_type, records in doctype_records.items():
        print('Indexing', len(records), index + '/' + doc_type, 'records.')
        try:
            elasticsearch.helpers.bulk(conn, index_iterator(records, index, doc_type), chunk_size)
        except ElasticsearchException as e:
            print('Indexing failed for index {}, type {}, for reason: {}'.format(index, doc_type, e), file=sys.stderr)
            continue
        print('Done indexing', len(records), index + '/' + doc_type, 'records.')

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Regenerate index based on the existing data to update the index types.')
    parser.add_argument('index', choices=['transfers', 'aips'], help='Index to re-generate.')
    parser.add_argument('--chunk-size', type=int, default=50, help='Number of docs in one chunk sent to Elasticsearch.')
    args = parser.parse_args()

    index_doctypes = {
        'transfers': ['transfer', 'transferfile'],
        'aips': ['aip', 'aipfile'],
    }

    sys.exit(reindex(args.index, index_doctypes[args.index], args.chunk_size))
